{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4883181",
   "metadata": {},
   "source": [
    "Import trained models and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eab0b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32917d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('cuda available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('cuda not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da146a4",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = ['9137e2b725024cb78e0fe4539220153a',\n",
    "           '2ad9e9608635496ca44f7e28871362c0',\n",
    "           '972e508debbd44b58acaf891562fcfde',\n",
    "           'f97e115a81224afda639999106868c0f',\n",
    "           '8a60045c8d5a4335a2a12c868c1cf0d4',\n",
    "           '3cfd69d12c974b14a3b60bf5ebe73cf7']  # better bigger dataset, fixed upscaling\n",
    "# various combinations of symmetric coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21890e0",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r'path/to/data/folder'  # TODO: set this to your local data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# %%\n",
    "dataset = 'AllenCahn_data_periodic_2000_2.pkl'\n",
    "data_path = os.path.join(data_folder, dataset)\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "N_samples = len(data['solutions'])\n",
    "inds = rng.choice(N_samples, size=3*N_samples//4, replace=False)\n",
    "bools = np.zeros(N_samples, dtype=bool)\n",
    "bools[inds] = True\n",
    "y_train = data['solutions'][bools]\n",
    "y_test = data['solutions'][~bools]\n",
    "\n",
    "x = torch.stack((data['epsilon'], data['mu']), axis=1)\n",
    "x_train = x[bools]\n",
    "x_test = x[~bools]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ebbe2",
   "metadata": {},
   "source": [
    "# Calculate residual before subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87028075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=64, shuffle=False)\n",
    "\n",
    "from scipy.sparse import diags\n",
    "L = 1.0\n",
    "N = y_train.shape[2]  # Number of spatial points\n",
    "\n",
    "dx = L / N        # Spatial step size\n",
    "x = np.linspace(0, L, N, endpoint=False)\n",
    "dt = 0.1\n",
    "T = y_train.shape[1]  # Number of time steps\n",
    "\n",
    "# Construct Laplacian with periodic boundary conditions\n",
    "main_diag = -2.0 * np.ones(N)\n",
    "off_diag = np.ones(N - 1)\n",
    "laplacian = diags([off_diag, main_diag, off_diag], offsets=[-1, 0, 1], shape=(N, N)).toarray()\n",
    "laplacian[0, -1] = laplacian[-1, 0] = 1.0  # periodic BC\n",
    "laplacian = laplacian / dx**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be43fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "residual_real = []\n",
    "with torch.no_grad():\n",
    "    # set random seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    for j, [epsmu, target] in enumerate(test_loader):\n",
    "        # print(f'Batch {j}')\n",
    "\n",
    "        cond = epsmu.to(device)\n",
    "\n",
    "        target = target.numpy()\n",
    "\n",
    "        # Allen-Cahn equation: du/dt = eps^2 * laplacian u - (u^3 - mu*u)\n",
    "\n",
    "        eps = cond[:, 0].reshape(-1, 1, 1).cpu().numpy()\n",
    "        mu = cond[:, 1].reshape(-1, 1, 1).cpu().numpy()\n",
    "\n",
    "        lhs2 = (target[:, 1:] - target[:, :-1])/dt\n",
    "        rhs2 = eps**2 * np.einsum('ij,kmj->kmi', laplacian, target[:, 1:]) - (target[:, :-1]**3 - mu * target[:, :-1])\n",
    "        diff2 = lhs2 - rhs2  # shape [B, T-1, N]\n",
    "        residual_real.extend(np.linalg.norm(diff2, axis=(1,2)).tolist())\n",
    "\n",
    "        ind = 2\n",
    "        if j == 1:\n",
    "            plt.imshow(lhs2[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'LHS du/dt, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(rhs2[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'RHS du/dt, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(target[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'Target, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            # break  # only first batch for testing\n",
    "\n",
    "\n",
    "print('Residual (Real):', np.mean(residual_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e2fb6",
   "metadata": {},
   "source": [
    "# Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 10\n",
    "\n",
    "if subsample is not None:\n",
    "    y_train = y_train[:, ::subsample]\n",
    "    y_test = y_test[:, ::subsample]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = torch.mean(y_train, dim=(0, 2))\n",
    "target_std = torch.std(y_train, dim=(0, 2))\n",
    "print('target_mean:', target_mean[::10])\n",
    "print('target_std:', target_std[::10])\n",
    "\n",
    "target_std = target_std.reshape(1, 1, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79dce30",
   "metadata": {},
   "source": [
    "# Calculate residual after subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=64, shuffle=False)\n",
    "\n",
    "from scipy.sparse import diags\n",
    "L = 1.0\n",
    "N = y_train.shape[2]  # Number of spatial points\n",
    "\n",
    "dx = L / N        # Spatial step size\n",
    "x = np.linspace(0, L, N, endpoint=False)\n",
    "dt = 1.0\n",
    "T = y_train.shape[1]  # Number of time steps\n",
    "\n",
    "# Construct Laplacian with periodic boundary conditions\n",
    "main_diag = -2.0 * np.ones(N)\n",
    "off_diag = np.ones(N - 1)\n",
    "laplacian = diags([off_diag, main_diag, off_diag], offsets=[-1, 0, 1], shape=(N, N)).toarray()\n",
    "laplacian[0, -1] = laplacian[-1, 0] = 1.0  # periodic BC\n",
    "laplacian = laplacian / dx**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "residual_real = []\n",
    "with torch.no_grad():\n",
    "    # set random seed\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    for j, [epsmu, target] in enumerate(test_loader):\n",
    "        # print(f'Batch {j}')\n",
    "\n",
    "        cond = epsmu.to(device)\n",
    "\n",
    "        target = target.numpy()\n",
    "\n",
    "        # Allen-Cahn equation: du/dt = eps^2 * laplacian u - (u^3 - mu*u)\n",
    "\n",
    "        eps = cond[:, 0].reshape(-1, 1, 1).cpu().numpy()\n",
    "        mu = cond[:, 1].reshape(-1, 1, 1).cpu().numpy()\n",
    "\n",
    "        lhs2 = (target[:, 1:] - target[:, :-1])/dt\n",
    "        rhs2 = eps**2 * np.einsum('ij,kmj->kmi', laplacian, target[:, 1:]) - (target[:, :-1]**3 - mu * target[:, :-1])\n",
    "        diff2 = lhs2 - rhs2  # shape [B, T-1, N]\n",
    "        residual_real.extend(np.linalg.norm(diff2, axis=(1,2)).tolist())\n",
    "\n",
    "        ind = 2\n",
    "        if j == 1:\n",
    "            plt.imshow(lhs2[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'LHS du/dt, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(rhs2[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'RHS du/dt, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(target[ind].T, aspect='auto', cmap='coolwarm')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'Target, eps={eps[ind][0][0]:3f}, mu={mu[ind][0][0]:3f}')\n",
    "            plt.show()\n",
    "\n",
    "            # break  # only first batch for testing\n",
    "\n",
    "\n",
    "print('Residual (Real):', np.mean(residual_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45396d",
   "metadata": {},
   "source": [
    "# Define UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3837380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2D_halfperiodic(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32, num_layers=4, kernel_size=3, resize_factor=2):\n",
    "        super(UNet2D_halfperiodic, self).__init__()\n",
    "        self.depth = num_layers\n",
    "\n",
    "        if isinstance(resize_factor, int):\n",
    "            self.resize_factor = [resize_factor]*num_layers\n",
    "        else:\n",
    "            assert len(resize_factor) == num_layers, \"resize_factor must be an int or a list of length num_layers\"\n",
    "            self.resize_factor = resize_factor\n",
    "\n",
    "        # Cheatsheet:\n",
    "        # Input size = [batch, in_channels, height=time steps, width=space points]\n",
    "\n",
    "        # Encoder path\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        prev_channels = in_channels\n",
    "        for i in range(num_layers):\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(prev_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            self.pools.append(nn.MaxPool2d(kernel_size=self.resize_factor[i], stride=self.resize_factor[i]))\n",
    "            prev_channels = hidden_channels\n",
    "\n",
    "        self.middle_conv = nn.Sequential(\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder path\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # After concatenation, channels = hidden_channels * 2 except at final\n",
    "            in_ch = hidden_channels * 2  # if i < num_layers - 1 else hidden_channels\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(in_ch, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "                    nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.extra_conv = nn.Sequential(\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "                    nn.CircularPad2d((1, 1, 0, 0)),\n",
    "                    nn.ConstantPad2d((0, 0, 1, 1), 0.0),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final mapping\n",
    "        self.final_convs = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels + in_channels, hidden_channels, kernel_size=1, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, padding='valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=1, padding='valid'),\n",
    "        )\n",
    "\n",
    "    def forward(self, tau, x, verbose=False):\n",
    "        # Encoder\n",
    "        x_in = x.clone()\n",
    "        skips = []\n",
    "        if verbose:\n",
    "            print(f'{\"Input shape         :\":<30} {x.shape}')\n",
    "        for encoder, pool in zip(self.encoders, self.pools):\n",
    "            x = encoder(x)\n",
    "            skips.append(x)\n",
    "            if verbose:\n",
    "                print(f'{\"Skip shape          :\":<30} {x.shape}')\n",
    "\n",
    "            x = pool(x)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'{\"Encoder output shape:\":<30} {x.shape}')\n",
    "\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'{\"Middle conv shape   :\":<30} {x.shape}')\n",
    "\n",
    "        # Bottleneck output\n",
    "        # Decoder\n",
    "        for i in range(0, len(self.decoders)):\n",
    "            # for periodic interpolation, first add periodic padding on both sides, then interpolate to needed size + 4, then remove extra element\n",
    "            x = F.pad(x, (1, 1, 0, 0), mode='circular')\n",
    "            target_size = [skips[-1].shape[2], skips[-1].shape[3]+4]\n",
    "            x = F.interpolate(x, size=target_size, mode='bilinear')[..., 2:-2]\n",
    "            if verbose:\n",
    "                print(f'{\"Decoder interpolated shape :\":<30} {x.shape}')\n",
    "\n",
    "            # Skip connection: concatenate with corresponding encoder output\n",
    "            skip = skips.pop()\n",
    "\n",
    "            x = torch.cat([x, skip], dim=1)  # if i < len(self.decoders) - 1 else x\n",
    "            x = self.decoders[i](x)\n",
    "            if verbose:\n",
    "                print(f'{\"Decoder output shape :\":<30} {x.shape}')\n",
    "\n",
    "        x = self.extra_conv(x)\n",
    "        x = torch.cat((x, x_in), dim=1)  # concatenate with input for final mapping\n",
    "        x = self.final_convs(x)  # final mapping to output channels\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c540457",
   "metadata": {},
   "source": [
    "# Define Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        super(Flow, self).__init__()\n",
    "        self.Unet = UNet2D_halfperiodic(\n",
    "            in_channels=4,\n",
    "            out_channels=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x_t, cond) -> Tensor:\n",
    "        # t.shape = [batch size, 1]\n",
    "        # x_t.shape = [batch size, 1, time steps, # points]\n",
    "        # cond.shape = [batch size, 2]\n",
    "\n",
    "        # print('forward: t.shape', t.shape)\n",
    "        # print('forward: x_t.shape', x_t.shape)\n",
    "        # print('forward: cond.shape', cond.shape)\n",
    "\n",
    "        inp = torch.cat((\n",
    "            x_t.clone(),\n",
    "            cond.reshape(-1,2,1,1).expand(-1, 2, x_t.shape[2], x_t.shape[3]),\n",
    "            t.reshape(-1,1,1,1).expand(-1, 1, x_t.shape[2], x_t.shape[3])\n",
    "        ), axis=1)\n",
    "\n",
    "        # print('forward: inp.shape', inp.shape)\n",
    "\n",
    "        return self.Unet(t, inp, verbose=False)\n",
    "\n",
    "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor, cond: Tensor) -> Tensor:\n",
    "        # t: float, current time\n",
    "        # t_end: float, end time\n",
    "        # x_t: shape [batch size, 1, time steps, # points], current interpolation\n",
    "        # cond: shape [batch size, 2], conditioning in the form of mu and epsilon\n",
    "        t_start = t_start.view(1, 1, 1, 1).expand(x_t.shape[0], 1, 1, 1)\n",
    "        t_end = t_end.view(1, 1, 1, 1).expand(x_t.shape[0], 1, 1, 1)\n",
    "\n",
    "        # print('step: x_t.shape', x_t.shape)\n",
    "        # print('step: t_start.shape', t_start.shape)\n",
    "        # print('step: t_end.shape', t_end.shape)\n",
    "        # print('step: cond.shape', cond.shape)\n",
    "\n",
    "        return (x_t + (t_end - t_start)\n",
    "                * self(\n",
    "                    t=t_start + (t_end - t_start) / 2,\n",
    "                    x_t= x_t + self(x_t=x_t, t=t_start, cond=cond) * (t_end - t_start) / 2,\n",
    "                    cond=cond\n",
    "                       )\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acc068",
   "metadata": {},
   "source": [
    "# Test Flow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags\n",
    "L = 1.0\n",
    "N = y_train.shape[2]  # Number of spatial points\n",
    "\n",
    "dx = L / N        # Spatial step size\n",
    "x = np.linspace(0, L, N, endpoint=False)\n",
    "dt = 1.0\n",
    "T = y_train.shape[1]  # Number of time steps\n",
    "\n",
    "# Construct Laplacian with periodic boundary conditions\n",
    "main_diag = -2.0 * np.ones(N)\n",
    "off_diag = np.ones(N - 1)\n",
    "laplacian = diags([off_diag, main_diag, off_diag], offsets=[-1, 0, 1], shape=(N, N)).toarray()\n",
    "laplacian[0, -1] = laplacian[-1, 0] = 1.0  # periodic BC\n",
    "laplacian = laplacian / dx**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc403e5",
   "metadata": {},
   "source": [
    "## Calculate residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cef839",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for run_id in run_ids:\n",
    "    print('run_id:', run_id)\n",
    "    flow = Flow().to(device)\n",
    "\n",
    "    # find the files with the weights of the model and the parameters needed to initialize it\n",
    "    art_path = mlflow.get_run(run_id).info.artifact_uri[8:]\n",
    "    print(art_path)\n",
    "    artifacts = client.list_artifacts(run_id)\n",
    "    for artifact in artifacts:\n",
    "        if 'model_weights.pt' in artifact.path:\n",
    "            model_path = os.path.join(art_path, artifact.path)\n",
    "            print(model_path)\n",
    "\n",
    "    flow.load_state_dict(torch.load(model_path))\n",
    "    print(flow)\n",
    "    flow.to(device)\n",
    "\n",
    "    # get mlflow parameter apply_gaussian_filter\n",
    "    apply_gaussian_filter = (client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true')\n",
    "    try:\n",
    "        sigma = float(client.get_run(run_id).data.params['sigma'])\n",
    "        print('sigma:', sigma)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "    flow.eval()\n",
    "\n",
    "    results[run_id] = {}\n",
    "\n",
    "    # Evaluate model for various nr of FM steps\n",
    "    for n_steps in [4, 16, 64, 256]:\n",
    "        print('n_steps:', n_steps)\n",
    "        time_steps = torch.linspace(0, 1.0, n_steps + 1).to(device)\n",
    "\n",
    "        residual_real = []\n",
    "        residual_pred = []\n",
    "        with torch.no_grad():\n",
    "            # set random seed\n",
    "            torch.manual_seed(42)\n",
    "\n",
    "            for j, [epsmu, target] in enumerate(test_loader):\n",
    "                # print(f'Batch {j}')\n",
    "\n",
    "                cond = epsmu.to(device)\n",
    "\n",
    "                # Initial guess\n",
    "                x = torch.randn(len(cond), 1, T, N, device=device) # initial Gaussian noise\n",
    "                x = torch.cumsum(x, dim=2)  # cumulative sum over time\n",
    "                # print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "                if apply_gaussian_filter:\n",
    "                    # print('here')\n",
    "                    # print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "                    # print('here')\n",
    "                    x = torch.as_tensor(gaussian_filter(x.cpu().numpy(), sigma=sigma, mode='wrap', axes=-1), dtype=torch.float32).to(device)  # apply Gaussian filter to make neighboring points more similar\n",
    "                # mean random walk displacement for each time\n",
    "                mean_disp = 1.0*torch.sqrt(2*(torch.arange(T, dtype=torch.float32, device=device)+1)/torch.pi)  # ðœŽâˆš(2ð‘/ðœ‹)\n",
    "                x = x / mean_disp.unsqueeze(-1) * target_std.to(device)  # scale to match target std\n",
    "\n",
    "                for i in range(n_steps):\n",
    "                    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1], cond=cond)\n",
    "\n",
    "                target = target.numpy()\n",
    "                x = x[:, 0].cpu().numpy()\n",
    "\n",
    "                # Allen-Cahn equation: du/dt = eps^2 * laplacian u - (u^3 - mu*u)\n",
    "\n",
    "                eps = cond[:, 0].reshape(-1, 1, 1).cpu().numpy()\n",
    "                mu = cond[:, 1].reshape(-1, 1, 1).cpu().numpy()\n",
    "\n",
    "                # laplacian shape [N, N]\n",
    "                # x.shape: [B, T, N]\n",
    "                # cond.shape: [B,]\n",
    "                lhs1 = (x[:, 1:] - x[:, :-1])/dt\n",
    "                rhs1a = eps**2 * np.einsum('ij,kmj->kmi', laplacian, x[:, 1:])\n",
    "                rhs1b = - (x[:, :-1]**3 - mu * x[:, :-1])\n",
    "                diff1 = lhs1 - (rhs1a + rhs1b)  # shape [B, T-1, N]\n",
    "                residual_pred.extend(np.linalg.norm(diff1, axis=(1,2)).tolist())\n",
    "\n",
    "                lhs2 = (target[:, 1:] - target[:, :-1])/dt\n",
    "                rhs2 = eps**2 * np.einsum('ij,kmj->kmi', laplacian, target[:, 1:]) - (target[:, :-1]**3 - mu * target[:, :-1])\n",
    "                diff2 = lhs2 - rhs2  # shape [B, T-1, N]\n",
    "                residual_real.extend(np.linalg.norm(diff2, axis=(1,2)).tolist())\n",
    "\n",
    "        print('Residual (Real):', np.mean(residual_real))\n",
    "        print('Residual (Predicted):', np.mean(residual_pred))\n",
    "\n",
    "        results[run_id][n_steps] = np.mean(residual_pred)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d227bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_id in results:\n",
    "    print('run_id:', run_id)\n",
    "    SM1 = client.get_run(run_id).data.params['symmetric_matching'].lower() == 'true'\n",
    "    SM3 = client.get_run(run_id).data.params['do_reflect_and_invert'].lower() == 'true'\n",
    "    gf = client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true'\n",
    "\n",
    "    print('symmetric_matching:', SM1)\n",
    "    print('do_reflect_and_invert:', SM3)\n",
    "    print('apply_gaussian_filter:', gf)\n",
    "\n",
    "    if SM3: name = f\"SM3, {'gf' if gf else 'no gf'}\"\n",
    "    elif SM1: name = f\"SM1, {'gf' if gf else 'no gf'}\"\n",
    "    else: name = f\"No SM, {'gf' if gf else 'no gf'}\"\n",
    "    print(name)\n",
    "\n",
    "    # if gf: continue\n",
    "\n",
    "    plt.plot(results[run_id].keys(), results[run_id].values(), label=name)\n",
    "\n",
    "    print(\"-------------\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Number of FM steps')\n",
    "plt.ylabel('Residual')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xticks([4, 16, 64, 256], [4, 16, 64, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3416b",
   "metadata": {},
   "source": [
    "## Make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e575e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make images of several predictions from each model\n",
    "\n",
    "results = {}\n",
    "for run_id in run_ids:\n",
    "    print('run_id:', run_id)\n",
    "    flow = Flow().to(device)\n",
    "\n",
    "    # find the files with the weights of the model and the parameters needed to initialize it\n",
    "    art_path = mlflow.get_run(run_id).info.artifact_uri[8:]\n",
    "    # print(art_path)\n",
    "    artifacts = client.list_artifacts(run_id)\n",
    "    for artifact in artifacts:\n",
    "        if 'model_weights.pt' in artifact.path:\n",
    "            model_path = os.path.join(art_path, artifact.path)\n",
    "            print(model_path)\n",
    "\n",
    "    flow.load_state_dict(torch.load(model_path))\n",
    "    # print(flow)\n",
    "    flow.to(device)\n",
    "\n",
    "    # get mlflow parameter apply_gaussian_filter\n",
    "    apply_gaussian_filter = (client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true')\n",
    "    try:\n",
    "        sigma = float(client.get_run(run_id).data.params['sigma'])\n",
    "        # print('sigma:', sigma)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "    flow.eval()\n",
    "\n",
    "    results[run_id] = {}\n",
    "\n",
    "    n_steps = 64\n",
    "    # print('n_steps:', n_steps)\n",
    "    time_steps = torch.linspace(0, 1.0, n_steps + 1).to(device)\n",
    "\n",
    "    SM1 = client.get_run(run_id).data.params['symmetric_matching'].lower() == 'true'\n",
    "    SM3 = client.get_run(run_id).data.params['do_reflect_and_invert'].lower() == 'true'\n",
    "    gf = client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true'\n",
    "\n",
    "    if SM3: name = f\"SM3, {'gf' if gf else 'no gf'}\"\n",
    "    elif SM1: name = f\"SM1, {'gf' if gf else 'no gf'}\"\n",
    "    else: name = f\"No SM, {'gf' if gf else 'no gf'}\"\n",
    "    print(name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # set random seed\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for j, [epsmu, target] in enumerate(test_loader):\n",
    "            print(f'Batch {j}')\n",
    "\n",
    "            cond = epsmu.to(device)\n",
    "\n",
    "            # Initial guess\n",
    "            x = torch.randn(len(cond), 1, T, N, device=device) # initial Gaussian noise\n",
    "            x = torch.cumsum(x, dim=2)  # cumulative sum over time\n",
    "            print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "            if apply_gaussian_filter:\n",
    "                print('here')\n",
    "                print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "                print('here')\n",
    "                x = torch.as_tensor(gaussian_filter(x.cpu().numpy(), sigma=sigma, mode='wrap', axes=-1), dtype=torch.float32).to(device)  # apply Gaussian filter to make neighboring points more similar\n",
    "            # mean random walk displacement for each time\n",
    "            mean_disp = 1.0*torch.sqrt(2*(torch.arange(T, dtype=torch.float32, device=device)+1)/torch.pi)  # ðœŽâˆš(2ð‘/ðœ‹)\n",
    "            x = x / mean_disp.unsqueeze(-1) * target_std.to(device)  # scale to match target std\n",
    "\n",
    "            for i in range(n_steps):\n",
    "                x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1], cond=cond)\n",
    "\n",
    "            target = target.numpy()\n",
    "            x = x[:, 0].cpu().numpy()\n",
    "\n",
    "            # Allen-Cahn equation: du/dt = eps^2 * laplacian u - (u^3 - mu*u)\n",
    "\n",
    "            eps = cond[:, 0].reshape(-1, 1, 1).cpu().numpy()\n",
    "            mu = cond[:, 1].reshape(-1, 1, 1).cpu().numpy()\n",
    "\n",
    "            for i in range(16):\n",
    "                if eps[i, 0, 0] > 0.1: continue\n",
    "                if mu[i, 0, 0] < 0.0: continue\n",
    "\n",
    "                fig, axes = plt.subplots(1,2,figsize=(6,2), dpi=200)\n",
    "                axes[0].imshow(\n",
    "                    x[i].T, aspect='auto', extent=[0, L, 0, T*subsample], origin='lower', vmin=-1, vmax=1, cmap='coolwarm'\n",
    "                )\n",
    "                axes[0].set_xlabel('time')\n",
    "                axes[0].set_ylabel('space')\n",
    "                fig.colorbar(axes[0].images[0], ax=axes[0])\n",
    "                axes[0].set_title(f'Predicted')\n",
    "\n",
    "                axes[1].imshow(\n",
    "                    target[i].T, aspect='auto', extent=[0, L, 0, T*subsample], origin='lower', vmin=-1, vmax=1, cmap='coolwarm'\n",
    "                )\n",
    "                axes[1].set_xlabel('time')\n",
    "                axes[1].set_ylabel('space')\n",
    "                fig.colorbar(axes[1].images[0], ax=axes[1])\n",
    "                axes[1].set_title(f'Target')\n",
    "\n",
    "                fig.suptitle(f'{name}, eps={eps[i,0,0]:.3f}, mu={mu[i,0,0]:.3f}')\n",
    "                fig.subplots_adjust(wspace=0.6, top=0.75)\n",
    "                plt.show()\n",
    "\n",
    "            if j == 1:\n",
    "                break\n",
    "\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592b165",
   "metadata": {},
   "source": [
    "# Bifurcation diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c3766",
   "metadata": {},
   "source": [
    "## Vary $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e46430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth\n",
    "with open('AllenCahn_periodic_varyMu.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "    print(data.keys())\n",
    "    solutions_gt = data['solutions']\n",
    "    mu_arr_gt = data['mu_arr']\n",
    "\n",
    "# Solution norm\n",
    "solution_avg = np.mean(solutions_gt[:, -1], axis=(-1))\n",
    "plt.scatter(mu_arr_gt, solution_avg, s=3, alpha=0.5)\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Solution norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72420f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "\n",
    "results = {}\n",
    "for run_id in run_ids:\n",
    "    print('run_id:', run_id)\n",
    "    flow = Flow().to(device)\n",
    "\n",
    "    # find the files with the weights of the model and the parameters needed to initialize it\n",
    "    art_path = mlflow.get_run(run_id).info.artifact_uri[8:]\n",
    "    # print(art_path)\n",
    "    artifacts = client.list_artifacts(run_id)\n",
    "    for artifact in artifacts:\n",
    "        if 'model_weights.pt' in artifact.path:\n",
    "            model_path = os.path.join(art_path, artifact.path)\n",
    "            print(model_path)\n",
    "\n",
    "    flow.load_state_dict(torch.load(model_path))\n",
    "    # print(flow)\n",
    "    flow.to(device)\n",
    "\n",
    "    # get mlflow parameter apply_gaussian_filter\n",
    "    apply_gaussian_filter = (client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true')\n",
    "    try:\n",
    "        sigma = float(client.get_run(run_id).data.params['sigma'])\n",
    "        # print('sigma:', sigma)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "    flow.eval()\n",
    "\n",
    "    n_steps = 64\n",
    "    # print('n_steps:', n_steps)\n",
    "    time_steps = torch.linspace(0, 1.0, n_steps + 1).to(device)\n",
    "\n",
    "    SM1 = client.get_run(run_id).data.params['symmetric_matching'].lower() == 'true'\n",
    "    SM3 = client.get_run(run_id).data.params['do_reflect_and_invert'].lower() == 'true'\n",
    "    gf = client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true'\n",
    "\n",
    "    if SM3: name = f\"SM3, {'gf' if gf else 'no gf'}\"\n",
    "    elif SM1: name = f\"SM1, {'gf' if gf else 'no gf'}\"\n",
    "    else: name = f\"No SM, {'gf' if gf else 'no gf'}\"\n",
    "    print(name)\n",
    "\n",
    "    avg_values = []\n",
    "    with torch.no_grad():\n",
    "        # set random seed\n",
    "        torch.manual_seed(42)\n",
    "        N_samples = 500\n",
    "        mu_arr = torch.linspace(-0.1, 1.0, N_samples, dtype=torch.float32).to(device)\n",
    "        eps_arr = torch.tensor([epsilon]*N_samples, dtype=torch.float32).to(device)\n",
    "        cond = torch.cat([eps_arr.unsqueeze(1), mu_arr.unsqueeze(1)], dim=1)\n",
    "\n",
    "        test_loader2 = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(cond),\n",
    "            batch_size=32,\n",
    "            shuffle=False\n",
    "        )\n",
    "        for j, cond_temp in enumerate(test_loader2):\n",
    "\n",
    "            cond_temp = cond_temp[0].to(device)\n",
    "\n",
    "            # Initial guess\n",
    "            x = torch.randn(len(cond_temp), 1, T, N, device=device) # initial Gaussian noise\n",
    "            x = torch.cumsum(x, dim=2)  # cumulative sum over time\n",
    "            if apply_gaussian_filter:\n",
    "                x = torch.as_tensor(gaussian_filter(x.cpu().numpy(), sigma=sigma, mode='wrap', axes=-1), dtype=torch.float32).to(device)  # apply Gaussian filter to make neighboring points more similar\n",
    "            # mean random walk displacement for each time\n",
    "            mean_disp = 1.0*torch.sqrt(2*(torch.arange(T, dtype=torch.float32, device=device)+1)/torch.pi)  # ðœŽâˆš(2ð‘/ðœ‹)\n",
    "            x = x / mean_disp.unsqueeze(-1) * target_std.to(device)  # scale to match target std\n",
    "\n",
    "            for i in range(n_steps):\n",
    "                # print(f'\\nStep {i}')\n",
    "                # print(x.shape)\n",
    "                x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1], cond=cond_temp)\n",
    "\n",
    "            avg_values.extend(x[..., 0, -1, :].mean(dim=-1).cpu().numpy())\n",
    "\n",
    "        mu_arr = torch.linspace(-0.1, 1.0, N_samples, dtype=torch.float32)\n",
    "\n",
    "        plt.figure(figsize=(4,3), dpi=200)\n",
    "        plt.scatter(mu_arr.cpu().numpy(), avg_values, label='Predicted', s=3, alpha=0.3)\n",
    "        plt.scatter(mu_arr_gt, solution_avg, label='Ground truth', s=3, zorder=-1)\n",
    "        plt.xlabel(r'$\\mu$')\n",
    "        plt.ylabel(r'Average solution value at final time')\n",
    "        plt.title(f'{name}, $\\epsilon={epsilon}$')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_arr = torch.linspace(-0.1, 1.0, N_samples, dtype=torch.float32)\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.scatter(mu_arr.cpu().numpy(), avg_values, label='Predicted', s=3, alpha=0.3)\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel(r'Average solution value at final time')\n",
    "plt.title(f'{name}, eps={epsilon}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd749c",
   "metadata": {},
   "source": [
    "## Vary $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c52d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load ground truth\n",
    "with open('AllenCahn_periodic_varyEps.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "    print(data.keys())\n",
    "    solutions_gt = data['solutions']\n",
    "    epsilon_arr_gt = data['epsilon_arr']\n",
    "\n",
    "# Solution norm\n",
    "solution_norms = np.linalg.norm(solutions_gt[:, -1], axis=(-1))\n",
    "plt.scatter(epsilon_arr_gt, solution_norms, s=3, alpha=0.5)\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('Solution norm')\n",
    "# plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "results = {}\n",
    "for run_id in run_ids:\n",
    "    print('run_id:', run_id)\n",
    "    flow = Flow().to(device)\n",
    "\n",
    "    # find the files with the weights of the model and the parameters needed to initialize it\n",
    "    art_path = mlflow.get_run(run_id).info.artifact_uri[8:]\n",
    "    # print(art_path)\n",
    "    artifacts = client.list_artifacts(run_id)\n",
    "    for artifact in artifacts:\n",
    "        if 'model_weights.pt' in artifact.path:\n",
    "            model_path = os.path.join(art_path, artifact.path)\n",
    "            print(model_path)\n",
    "\n",
    "    flow.load_state_dict(torch.load(model_path))\n",
    "    # print(flow)\n",
    "    flow.to(device)\n",
    "\n",
    "    # get mlflow parameter apply_gaussian_filter\n",
    "    apply_gaussian_filter = (client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true')\n",
    "    try:\n",
    "        sigma = float(client.get_run(run_id).data.params['sigma'])\n",
    "        # print('sigma:', sigma)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # print('apply_gaussian_filter:', apply_gaussian_filter)\n",
    "    flow.eval()\n",
    "\n",
    "    n_steps = 64\n",
    "    # print('n_steps:', n_steps)\n",
    "    time_steps = torch.linspace(0, 1.0, n_steps + 1).to(device)\n",
    "\n",
    "    SM1 = client.get_run(run_id).data.params['symmetric_matching'].lower() == 'true'\n",
    "    SM3 = client.get_run(run_id).data.params['do_reflect_and_invert'].lower() == 'true'\n",
    "    gf = client.get_run(run_id).data.params['apply_gaussian_filter'].lower() == 'true'\n",
    "\n",
    "    if SM3: name = f\"SM3, {'gf' if gf else 'no gf'}\"\n",
    "    elif SM1: name = f\"SM1, {'gf' if gf else 'no gf'}\"\n",
    "    else: name = f\"No SM, {'gf' if gf else 'no gf'}\"\n",
    "    print(name)\n",
    "\n",
    "    sol_norm = []\n",
    "    with torch.no_grad():\n",
    "        # set random seed\n",
    "        torch.manual_seed(42)\n",
    "        N_samples = 500\n",
    "        eps_arr = 10**torch.linspace(-3, -1, N_samples, dtype=torch.float32).to(device)\n",
    "        mu_arr = torch.tensor([1.0]*N_samples, dtype=torch.float32).to(device)\n",
    "        cond = torch.cat([eps_arr.unsqueeze(1), mu_arr.unsqueeze(1)], dim=1)\n",
    "\n",
    "        test_loader2 = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(cond),\n",
    "            batch_size=256,\n",
    "            shuffle=False\n",
    "        )\n",
    "        for j, cond_temp in enumerate(test_loader2):\n",
    "\n",
    "            cond_temp = cond_temp[0].to(device)\n",
    "\n",
    "            # Initial guess\n",
    "            x = torch.randn(len(cond_temp), 1, T, N, device=device) # initial Gaussian noise\n",
    "            x = torch.cumsum(x, dim=2)  # cumulative sum over time\n",
    "            if apply_gaussian_filter:\n",
    "                x = torch.as_tensor(gaussian_filter(x.cpu().numpy(), sigma=sigma, mode='wrap', axes=-1), dtype=torch.float32).to(device)  # apply Gaussian filter to make neighboring points more similar\n",
    "            # mean random walk displacement for each time\n",
    "            mean_disp = 1.0*torch.sqrt(2*(torch.arange(T, dtype=torch.float32, device=device)+1)/torch.pi)  # ðœŽâˆš(2ð‘/ðœ‹)\n",
    "            x = x / mean_disp.unsqueeze(-1) * target_std.to(device)  # scale to match target std\n",
    "\n",
    "            for i in range(n_steps):\n",
    "                # print(f'\\nStep {i}')\n",
    "                # print(x.shape)\n",
    "                x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1], cond=cond_temp)\n",
    "\n",
    "            sol_norm.extend(torch.norm(x[..., 0, -1, :], dim=-1).cpu().numpy())\n",
    "\n",
    "        plt.figure(figsize=(4,3), dpi=200)\n",
    "        plt.scatter(eps_arr.cpu().numpy(), sol_norm, label='Predicted', s=3, alpha=0.3)\n",
    "        plt.scatter(epsilon_arr_gt, solution_norms, label='Ground truth', s=3, zorder=-1)\n",
    "        plt.xlabel(r'$\\epsilon$')\n",
    "        plt.ylabel(r'Solution norm at final time')\n",
    "        plt.title(f'{name}, $\\mu=1.0$')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c750f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.scatter(eps_arr.cpu().numpy(), sol_norm, label='Predicted', s=3, alpha=0.3)\n",
    "plt.scatter(epsilon_arr_gt, solution_norms, label='Ground truth', s=3, zorder=-1)\n",
    "plt.xlabel(r'$\\epsilon$')\n",
    "plt.ylabel(r'Solution norm at final time')\n",
    "plt.title(f'{name}, $\\mu=1.0$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58185194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bifurc_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
