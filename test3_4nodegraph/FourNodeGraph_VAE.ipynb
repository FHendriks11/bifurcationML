{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Google Colab, uncomment this to install torch_geometric:\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as tg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/FourNodeGraph_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(key, value.size())\n",
    "    elif isinstance(value, list):\n",
    "        print(key, len(value))\n",
    "    else:\n",
    "        print(key, value)\n",
    "\n",
    "data_tr = data['data_tr']\n",
    "data_te = data['data_te']\n",
    "x_m = data['x_m']\n",
    "y_m = data['y_m']\n",
    "x_std = data['x_std']\n",
    "y_std = data['y_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tg.loader.DataLoader(data_tr, batch_size=64)\n",
    "test_loader = tg.loader.DataLoader(data_te, batch_size=100000) # all test data at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('cuda available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('cuda not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(logsig_i, mu_i, logsig_f=torch.tensor(0.0), mu_f=torch.tensor(0.0)):\n",
    "    # logsig_f and mu_f are optional, if not given, they are assumed to be 0\n",
    "    # (corresponding to a standard normal distribution, with mu=0 and sig=1)\n",
    "    temp = 2*(logsig_f-logsig_i) - 1 + torch.exp(logsig_i)**2/torch.exp(logsig_f)**2 + (mu_f - mu_i)**2/torch.exp(logsig_f)**2\n",
    "    temp = 0.5*torch.sum(temp, axis=-1)\n",
    "    return torch.mean(temp)\n",
    "\n",
    "def MSELoss_allTargets(pred, target, batch, return_indices=False):\n",
    "    \"\"\"MSE loss applied on all possible targets, then taking the minimum per graph\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : torch tensor, [N, f]\n",
    "        prediction per node, N = total nr of nodes, f = nr of predicted features\n",
    "    target : torch tensor, [N, f, a]\n",
    "        all possible targets, N = total nr of nodes, f = nr of predicted features, a = nr of possible alternatives\n",
    "    batch : torch tensor, [N, ]\n",
    "        graph that each node belongs to\n",
    "    return_indices : bool, optional\n",
    "        whether to return the indices of the alternatives giving the minimum loss, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch tensor, [1,]\n",
    "        MSE loss, using the closest alternative for each graph\n",
    "    \"\"\"\n",
    "    # sum square error per node per alternative (resulting shape: [N, a])\n",
    "    SE = torch.sum((pred.unsqueeze(-1) - target)**2, dim=1)\n",
    "    # sum square error per graph per alternative (resulting shape: [G, a], with G the nr of graphs)\n",
    "    SE = tg.nn.global_add_pool(SE, batch=batch)\n",
    "    # take the minimum error per graph\n",
    "    SE_min, inds = torch.min(SE, axis=-1)\n",
    "\n",
    "    # take mean over entire batch\n",
    "    MSE = torch.sum(SE_min)/(pred.numel())\n",
    "\n",
    "    if return_indices:\n",
    "        return MSE, inds\n",
    "    else:\n",
    "        return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMessagePassingLayer(MessagePassing):\n",
    "    \"\"\"message passing layer that updates both node and edge embeddings.\n",
    "    first, the node embedding is updated based on the previous node embedding and the message received from neighboring nodes. these messages are based on the embedding of the source node and the edge attributes.\n",
    "    second, the edge embedding is updated based on the previous edge embedding and the node embeddings of the source and target node.\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    messagepassing : [type]\n",
    "        [description]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_in, edge_in, message_size, node_out, edge_out):\n",
    "\n",
    "        \"\"\"initialize layer\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        node_in : int\n",
    "            previous node embedding size\n",
    "        edge_in: int\n",
    "            previous edge embedding size\n",
    "        message_size : int\n",
    "            size of the message\n",
    "        node_out : int\n",
    "            node embedding size after updating\n",
    "        edge_out: inL\n",
    "            edge embedding size after updating\n",
    "        \"\"\"\n",
    "        super().__init__(aggr='add')\n",
    "        self.mlp_message = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(2*node_in + edge_in, message_size),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(message_size, message_size),\n",
    "                        torch.nn.LeakyReLU())\n",
    "        self.mlp_update = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(node_in + message_size,\n",
    "                                        node_out),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(node_out, node_out))\n",
    "        self.mlp_edge = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(edge_in + 2*node_in, edge_out),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(edge_out, edge_out))\n",
    "\n",
    "        # print('__init__')\n",
    "        # self.lin1 = torch.nn.linear(node_in, node_out)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        x : torch.tensor, shape [n, node_in]\n",
    "            current node embedding for each of the n nodes\n",
    "        edge_index : torch.tensor, shape [2, e]\n",
    "            indices of all edges\n",
    "        edge_attr : torch.tensor, shape [e, edge_in]\n",
    "            edge_attributes of each of the e edges\n",
    "        \"\"\"\n",
    "        # print('forward')\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr), self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
    "        # print('edge_updater')\n",
    "        temp = torch.cat((edge_attr, x_i, x_j), dim=1)\n",
    "        # print('edge_update, temp.shape:', temp.shape)\n",
    "\n",
    "        return self.mlp_edge(temp)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        x_j : torch.tensor, shape [e, node_in]\n",
    "            node embeddings of source nodes\n",
    "        edge_attr : torch.tensor, shape [e, edge_in]\n",
    "            [description]\n",
    "        \"\"\"\n",
    "        # print('message')\n",
    "        # print('x_j.shape, edge_attr.shape:', x_j.shape, edge_attr.shape)\n",
    "        temp = torch.cat((x_i, x_j, edge_attr), dim=1)\n",
    "        return self.mlp_message(temp)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # print('update')\n",
    "        # print('x.shape, aggr_out.shape:', x.shape, aggr_out.shape)\n",
    "        temp = torch.cat((x, aggr_out), dim=1)\n",
    "        # temp = x\n",
    "        return self.mlp_update(temp)\n",
    "\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, node_in, node_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = MyMessagePassingLayer(node_in, 0, 0, 32, 32)\n",
    "        self.conv2 = MyMessagePassingLayer(32, 32, 32, node_out, 0)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "\n",
    "        # convolutional part\n",
    "        x, edge_attr = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x, edge_attr = F.leaky_relu(x), F.leaky_relu(edge_attr)\n",
    "        x, _ = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ProbabilisticGNN1(torch.nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embeddingGNN1 = SimpleGNN(1, 32)\n",
    "        self.embeddingGNN2 = SimpleGNN(2, 32)\n",
    "        self.mlp1 = torch.nn.Sequential(\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 2*latent_dim)\n",
    "                        )\n",
    "        self.mlp2 = torch.nn.Sequential(\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32*2, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 2*latent_dim)\n",
    "                        )\n",
    "        self.finalGNN = SimpleGNN(32 + latent_dim, 1)\n",
    "\n",
    "    def forward(self, data, cond_on_final=False):\n",
    "        x, y, edge_index, edge_attr = data.x, data.y, data.edge_index, data.edge_attr\n",
    "\n",
    "        if y.ndim == 3:  # if multiple options are provided, take the first one\n",
    "            y = y[..., 0]\n",
    "        elif y.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f'ground truth output y should have 2 or 3 dimensions, not {y.ndim}')\n",
    "\n",
    "        # Apply GNN to initial\n",
    "        x2 = self.embeddingGNN1(x.clone(), edge_index, edge_attr)\n",
    "        if cond_on_final:\n",
    "            # Apply GNN to final\n",
    "            x3 = self.embeddingGNN2(torch.cat((x.clone(), y.clone()), dim=-1), edge_index, edge_attr)\n",
    "\n",
    "        # Create logsig, mu from initial\n",
    "        logsig_mu_i = self.mlp1(x2).reshape(-1, self.latent_dim, 2)\n",
    "        logsig_i, mu_i = logsig_mu_i[..., 0], logsig_mu_i[..., 1]\n",
    "        if cond_on_final:\n",
    "            # Create logsig, mu from initial and final\n",
    "            x4 = torch.cat((x2, x3), dim=-1)\n",
    "            logsig_mu_f = self.mlp2(x4).reshape(-1, self.latent_dim, 2)\n",
    "            logsig_f, mu_f = logsig_mu_f[..., 0], logsig_mu_f[..., 1]\n",
    "\n",
    "        # Sample\n",
    "        eps = torch.randn_like(logsig_i)\n",
    "        z_i = eps*torch.exp(logsig_i) + mu_i\n",
    "        if cond_on_final:\n",
    "            eps = torch.randn_like(logsig_f)\n",
    "            z_f = eps*torch.exp(logsig_f) + mu_f\n",
    "\n",
    "        if cond_on_final:\n",
    "            x = torch.cat((x2, z_f), dim=-1)\n",
    "        else:\n",
    "            x = torch.cat((x2, z_i), dim=-1)\n",
    "\n",
    "        x = self.finalGNN(x, edge_index, edge_attr)\n",
    "\n",
    "        if cond_on_final:\n",
    "            return logsig_i, mu_i, logsig_f, mu_f, x\n",
    "        else:\n",
    "            return (x, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProbabilisticGNN1(latent_dim=8).to(device=device)\n",
    "print(model)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total nr of parameters:', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GNN before training\n",
    "To test the initialization of the network, evaluate the GNN before even training it. We use the 'MSELoss_allTargets' function, which calculates the mean square loss before the target and every prediction, and then picks the best prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    pred = model(batch)[-1]\n",
    "    test_loss, inds = MSELoss_allTargets(pred, batch.y, batch=batch.batch, return_indices=True)\n",
    "\n",
    "    ground_truth = np.take_along_axis(batch.y.cpu().detach().numpy().reshape(-1,4,1,2),\n",
    "                                    inds.cpu().numpy().reshape(-1,1,1,1), axis=-1)\n",
    "    plt.scatter(ground_truth, pred.cpu().detach().numpy(), s=1)\n",
    "    # plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('real')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.title('Real vs predicted before training\\n(to check initialization of network)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "ep_lower_lr = 0\n",
    "\n",
    "losses = {\n",
    "            # on training data, while conditioning on output:\n",
    "            'MSE_train': [],\n",
    "            'KL_train': [],\n",
    "            'loss_train': [], # total loss\n",
    "\n",
    "            # on validation data, without conditioning on output:\n",
    "            'MSE_val': [],\n",
    "            'MSE_val_allTargets': [],  # MSE compared to closest target\n",
    "\n",
    "            # on validation data, but still conditioned on output:\n",
    "            'KL_val2': [],\n",
    "            'MSE_val2': []\n",
    "        }\n",
    "\n",
    "KL_increase_range = 50 # nr of epochs over which we increase the weight of the KL loss\n",
    "for epoch in range(500):\n",
    "    print(f'epoch {epoch}')\n",
    "\n",
    "    # ====================== 1) TRAIN ======================\n",
    "    KL_weight = np.clip(epoch/KL_increase_range, a_min=0.0, a_max=1.0)/2\n",
    "    print(' \\t KL_weight:', KL_weight)\n",
    "    losses_temp = {key: [] for key in losses}\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = batch.clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(batch, cond_on_final=True)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch.y[..., 0])\n",
    "        KL_loss1 = KL_loss(*out[:-1])\n",
    "\n",
    "        loss = (1.0-KL_weight)*MSE_loss + KL_weight*KL_loss1\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_temp['MSE_train'].append(MSE_loss.item())\n",
    "        losses_temp['KL_train'].append(KL_loss1.item())\n",
    "        losses_temp['loss_train'].append(loss.item())\n",
    "\n",
    "    for loss in ['MSE_train', 'KL_train', 'loss_train']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # =============== 2) EVALUATE WITHOUT CONDITIONING ON OUTPUT ===============\n",
    "    # In evaluation mode, so not conditioned on output. Calculating KL loss is therefore not possible.\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone().to(device)\n",
    "        out = model(batch, cond_on_final=False)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch.y[..., 0])\n",
    "        MSE_loss_alltarg = MSELoss_allTargets(pred, batch.y, batch=batch.batch)\n",
    "\n",
    "        losses_temp['MSE_val'].append(MSE_loss.item())\n",
    "        losses_temp['MSE_val_allTargets'].append(MSE_loss_alltarg.item())\n",
    "\n",
    "    for loss in ['MSE_val', 'MSE_val_allTargets']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # ================ 3) EVALUATE WITH CONDITIONING ON OUTPUT ================\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone().to(device)\n",
    "        out = model(batch, cond_on_final=True)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch.y[..., 0])\n",
    "        KL_loss1 = KL_loss(*out[:-1])\n",
    "\n",
    "        losses_temp['MSE_val2'].append(MSE_loss.item())\n",
    "        losses_temp['KL_val2'].append(KL_loss1.item())\n",
    "\n",
    "    for loss in ['MSE_val2', 'KL_val2']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # ======== Early stopping =========\n",
    "    # Stop training if both MSE and KL loss did not decrease enough on validation data\n",
    "    if (\n",
    "        epoch > KL_increase_range\n",
    "        and\n",
    "        (np.mean(losses[\"MSE_val_allTargets\"][-5:])\n",
    "             > 0.995*np.mean(losses[\"MSE_val_allTargets\"][-10:-5]))\n",
    "        and (np.mean(losses[\"KL_val2\"][-5:])\n",
    "             > 0.995*np.mean(losses[\"KL_val2\"][-10:-5]))\n",
    "    ):\n",
    "\n",
    "        print('======= !! Validation loss did not decrease enough =======')\n",
    "        print('Stopping training')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for key, value in losses.items():\n",
    "    plt.plot(value, label=key)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare input, output, predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "    # print input\n",
    "    print('============== INPUT ==============')\n",
    "    temp_x = (batch.x[:6*4]*x_std + x_m).cpu().detach().numpy().astype(int)\n",
    "    for asdf in temp_x.reshape(-1, 4):\n",
    "        print(*asdf, sep=', ')\n",
    "    # print ground truth output\n",
    "    batch.y[:12]*y_std + y_m\n",
    "\n",
    "    print('============== POSSIBLE OUTPUTS ==============')\n",
    "    temp_y = (batch.y[:6*4]*y_std + y_m).cpu().detach().numpy()\n",
    "    for asdf in temp_y.reshape(-1, 4, 2):\n",
    "        print(*asdf.T, sep=', ')\n",
    "    # print predicted output\n",
    "    batch.y[:12]*y_std + y_m\n",
    "\n",
    "    print('============== PREDICTIONS ==============')\n",
    "    model.eval()\n",
    "    pred = model(batch)[-1][:6*4]*y_std + y_m\n",
    "    temp_y = pred.cpu().detach().numpy()\n",
    "    for asdf in temp_y.reshape(-1, 4):\n",
    "        print(*asdf.T, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot real vs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    pred = model(batch, cond_on_final=False)[-1]\n",
    "    print('Shapes:', pred.shape, batch.y.shape)\n",
    "    test_loss, inds = MSELoss_allTargets(pred, batch.y, batch=batch.batch, return_indices=True)\n",
    "    print('test RMSE', torch.sqrt(test_loss).item())\n",
    "\n",
    "    ground_truth = np.take_along_axis(batch.y.cpu().detach().numpy().reshape(-1,4,1,2),\n",
    "                                    inds.cpu().numpy().reshape(-1,1,1,1), axis=-1)\n",
    "    plt.scatter(ground_truth*y_std+y_m, pred.cpu().detach().numpy()*y_std+y_m, s=1)\n",
    "    plt.axline([0,0], [1,1], c='tab:orange')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('real')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.title('Real (best match) vs predicted, not conditioned on output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    pred = model(batch, cond_on_final=True)[-1]\n",
    "    print('Shapes:', pred.shape, batch.y.shape)\n",
    "    test_loss, inds = MSELoss_allTargets(pred, batch.y, batch=batch.batch, return_indices=True)\n",
    "    print('test RMSE', torch.sqrt(test_loss).item())\n",
    "\n",
    "    ground_truth = np.take_along_axis(batch.y.cpu().detach().numpy().reshape(-1,4,1,2),\n",
    "                                    inds.cpu().numpy().reshape(-1,1,1,1), axis=-1)\n",
    "    plt.axline([0,0], [1,1], c='tab:orange')\n",
    "    plt.scatter(ground_truth*y_std + y_m, pred.cpu().detach().numpy()*y_std + y_m, s=1)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('real')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.title('Real (best match) vs predicted, conditioned on output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bifurcation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot how ground truth bifurcates\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_facecolor(\"None\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    for ind in range(100):\n",
    "        pos1 = (batch.x[[4*ind+0, 4*ind+2],0]*x_std+x_m).cpu().detach().numpy()\n",
    "        pos2a = (batch.y[[4*ind+0, 4*ind+2],0,0]*y_std+y_m).cpu().detach().numpy()\n",
    "        pos2b = (batch.y[[4*ind+1, 4*ind+3],0,0]*y_std+y_m).cpu().detach().numpy()\n",
    "\n",
    "        ax.scatter(*pos1,  c='tab:blue',   s=10, label='Initial')\n",
    "        ax.scatter(*pos2a, c='tab:orange', s=10, label='Final, node 0 & 2')\n",
    "        ax.scatter(*pos2b, c='tab:red',    s=10, label='Final, node 1 & 3')\n",
    "\n",
    "        ax.annotate('', xy=pos2a, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "        ax.annotate('', xy=pos2b, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles[:3], labels[:3])\n",
    "    ax.set_xlabel('Embedding node 0 and 1')\n",
    "    ax.set_ylabel('Embedding node 2 and 3')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Ground truth bifurcation')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how result bifurcates (not conditioned on output)\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_facecolor(\"None\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device=device)\n",
    "    pred = model(batch, cond_on_final=False)[-1]\n",
    "\n",
    "    for ind in range(100):\n",
    "        pos1 = (batch.x[[4*ind+0, 4*ind+2]]*x_std+x_m).cpu().detach().numpy()\n",
    "        pos2a = (pred[[4*ind+0, 4*ind+2]]*y_std+y_m).cpu().detach().numpy()\n",
    "        pos2b = (pred[[4*ind+1, 4*ind+3]]*y_std+y_m).cpu().detach().numpy()\n",
    "\n",
    "        ax.scatter(*pos1,  c='tab:blue',   s=10, label='Initial')\n",
    "        ax.scatter(*pos2a, c='tab:orange', s=10, label='Final, node 0 & 2')\n",
    "        ax.scatter(*pos2b, c='tab:red',    s=10, label='Final, node 1 & 3')\n",
    "\n",
    "        ax.annotate('', xy=pos2a, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "        ax.annotate('', xy=pos2b, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles[:3], labels[:3])\n",
    "    ax.set_xlabel('Embedding node 0 and 1')\n",
    "    ax.set_ylabel('Embedding node 2 and 3')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Predicted bifurcation (not conditioned on output)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how result bifurcates (conditioned on output)\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_facecolor(\"None\")\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device=device)\n",
    "    pred = model(batch, cond_on_final=True)[-1]\n",
    "\n",
    "    for ind in range(100):\n",
    "        pos1 = (batch.x[[4*ind+0, 4*ind+2]]*x_std+x_m).cpu().detach().numpy()\n",
    "        pos2a = (pred[[4*ind+0, 4*ind+2]]*y_std+y_m).cpu().detach().numpy()\n",
    "        pos2b = (pred[[4*ind+1, 4*ind+3]]*y_std+y_m).cpu().detach().numpy()\n",
    "\n",
    "        ax.scatter(*pos1,  c='tab:blue',   s=10, label='Initial')\n",
    "        ax.scatter(*pos2a, c='tab:orange', s=10, label='Final, node 0 & 2')\n",
    "        ax.scatter(*pos2b, c='tab:red',    s=10, label='Final, node 1 & 3')\n",
    "\n",
    "        ax.annotate('', xy=pos2a, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "        ax.annotate('', xy=pos2b, xytext=pos1,\n",
    "                    arrowprops=dict(arrowstyle='->', facecolor='black'),\n",
    "                    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles[:3], labels[:3])\n",
    "    ax.set_xlabel('Embedding node 0 and 1')\n",
    "    ax.set_ylabel('Embedding node 2 and 3')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Predicted bifurcation (conditioned on output)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if bifurcation at least is consistent-ish\n",
    "If the predicted value of node 1 is higher than node 0, then the predicted value of node 3 should also be higher than node 2. This way we can check if the bifurcation is at least going in a correct direction, even if the actual prediction is not very accurate yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% check if bifurcation at least is consistent-ish\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "    pred = model(batch, cond_on_final=False)[-1]\n",
    "    pred = (pred*y_std+y_m).cpu().detach().numpy().reshape(-1, 4)\n",
    "\n",
    "    bools = (pred[:, 0] > pred[:, 1]) == (pred[:, 2] > pred[:, 3])\n",
    "    print(f'No conditioning on output: Bifurcation in the right direction {sum(bools)/len(bools)*100:.1f}% of the time')\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "    pred = model(batch, cond_on_final=True)[-1]\n",
    "    pred = (pred*y_std+y_m).cpu().detach().numpy().reshape(-1, 4)\n",
    "\n",
    "    bools = (pred[:, 0] > pred[:, 1]) == (pred[:, 2] > pred[:, 3])\n",
    "    print(f'Conditioned on output: Bifurcation in the right direction {sum(bools)/len(bools)*100:.1f}% of the time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance_nd\n",
    "model.eval()\n",
    "rel_dist = []\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "\n",
    "    preds = np.zeros((len(batch.x), 100))  # create 100 predictions per node\n",
    "\n",
    "    for i in range(100):  # make 100 predictions for each data point\n",
    "        preds[:, i] = model(batch, cond_on_final=False)[-1].cpu().detach().numpy().reshape(-1)*y_std + y_m\n",
    "\n",
    "    preds = preds.reshape(-1, 4, 100)\n",
    "    reals = batch.y.cpu().detach().numpy().reshape(-1, 4, 2)*y_std + y_m\n",
    "\n",
    "    print('preds.shape:', preds.shape)\n",
    "    print('reals.shape:', reals.shape)\n",
    "    wd = []\n",
    "    for pred, real in zip(preds, reals):  # iterate over all data points (as far as I know, this cannot be batched)\n",
    "        wd.append(wasserstein_distance_nd(pred.T, real.T))\n",
    "\n",
    "        diff = pred[..., np.newaxis] - real[..., np.newaxis, :]\n",
    "        dist = np.linalg.norm(diff, axis=0)\n",
    "        print(diff.shape)\n",
    "        print(dist.shape)\n",
    "        rel_dist_temp = dist[..., 0] / (dist[..., 0] + dist[..., 1])\n",
    "        print(rel_dist_temp.shape)\n",
    "        rel_dist.extend(rel_dist_temp.flatten().tolist())\n",
    "\n",
    "print(np.mean(wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of rel_dist\n",
    "# set font size to 16\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(rel_dist,\n",
    "         bins=np.linspace(0, 1, 40), density=True)\n",
    "plt.xlabel(r'$\\frac{\\text{Distance to option 1}}{\\text{Distance to option 1 + Distance to option 2}}$', fontsize=18)\n",
    "# change label font size x label\n",
    "plt.axvline(0.0, color='red', linestyle='--')\n",
    "plt.axvline(1.0, color='red', linestyle='--')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the VAE+GNN approach is only very slightly better than the non-probabilistic GNN approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bifurc_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
