{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Google Colab, uncomment this to install torch_geometric:\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/FourNodeGraph_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(key, value.size())\n",
    "    elif isinstance(value, list):\n",
    "        print(key, len(value))\n",
    "    else:\n",
    "        print(key, value)\n",
    "\n",
    "data_tr = data['data_tr']\n",
    "data_te = data['data_te']\n",
    "x_m = data['x_m']\n",
    "y_m = data['y_m']\n",
    "x_std = data['x_std']\n",
    "y_std = data['y_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tg.loader.DataLoader(data_tr, batch_size=64)\n",
    "test_loader = tg.loader.DataLoader(data_te, batch_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('cuda available')\n",
    "    # mlflow.log_param('device', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('cuda not available')\n",
    "    # mlflow.log_param('device', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMessagePassingLayer(MessagePassing):\n",
    "    \"\"\"message passing layer that updates both node and edge embeddings.\n",
    "    first, the node embedding is updated based on the previous node embedding and the message received from neighboring nodes. these messages are based on the embedding of the source node and the edge attributes.\n",
    "    second, the edge embedding is updated based on the previous edge embedding and the node embeddings of the source and target node.\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    messagepassing : [type]\n",
    "        [description]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_in, edge_in, message_size, node_out, edge_out):\n",
    "\n",
    "        \"\"\"initialize layer\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        node_in : int\n",
    "            previous node embedding size\n",
    "        edge_in: int\n",
    "            previous edge embedding size\n",
    "        message_size : int\n",
    "            size of the message\n",
    "        node_out : int\n",
    "            node embedding size after updating\n",
    "        edge_out: in\n",
    "            edge embedding size after updating\n",
    "        \"\"\"\n",
    "        super().__init__(aggr='add')\n",
    "        self.mlp_message = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(2*node_in + edge_in, message_size),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(message_size, message_size),\n",
    "                        torch.nn.LeakyReLU())\n",
    "        self.mlp_update = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(node_in + message_size,\n",
    "                                        node_out),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(node_out, node_out))\n",
    "        self.mlp_edge = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(edge_in + 2*node_in, edge_out),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(edge_out, edge_out))\n",
    "\n",
    "        # print('__init__')\n",
    "        # self.lin1 = torch.nn.linear(node_in, node_out)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        x : torch.tensor, shape [n, node_in]\n",
    "            current node embedding for each of the n nodes\n",
    "        edge_index : torch.tensor, shape [2, e]\n",
    "            indices of all edges\n",
    "        edge_attr : torch.tensor, shape [e, edge_in]\n",
    "            edge_attributes of each of the e edges\n",
    "        \"\"\"\n",
    "        # print('forward')\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr), self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def edge_update(self, edge_index, x_i, x_j, edge_attr):\n",
    "        # print('edge_updater')\n",
    "        temp = torch.cat((edge_attr, x_i, x_j), dim=1)\n",
    "        # print('edge_update, temp.shape:', temp.shape)\n",
    "\n",
    "        return self.mlp_edge(temp)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        parameters\n",
    "        ----------\n",
    "        x_j : torch.tensor, shape [e, node_in]\n",
    "            node embeddings of source nodes\n",
    "        edge_attr : torch.tensor, shape [e, edge_in]\n",
    "            [description]\n",
    "        \"\"\"\n",
    "        # print('message')\n",
    "        # print('x_j.shape, edge_attr.shape:', x_j.shape, edge_attr.shape)\n",
    "        temp = torch.cat((x_i, x_j, edge_attr), dim=1)\n",
    "        return self.mlp_message(temp)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # print('update')\n",
    "        # print('x.shape, aggr_out.shape:', x.shape, aggr_out.shape)\n",
    "        temp = torch.cat((x, aggr_out), dim=1)\n",
    "        # temp = x\n",
    "        return self.mlp_update(temp)\n",
    "\n",
    "\n",
    "class MyGNN(torch.nn.Module):\n",
    "    def __init__(self, use_edge_weight=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = MyMessagePassingLayer(1, 0, 0, 32, 32)\n",
    "        # self.batchnorm_x = torch.nn.BatchNorm1d(32)\n",
    "        # self.batchnorm_e = torch.nn.BatchNorm1d(32)\n",
    "        self.conv2 = MyMessagePassingLayer(32, 32, 32, 32, 32)\n",
    "        # self.batchnorm2_x = torch.nn.BatchNorm1d(32)\n",
    "        # self.batchnorm2_e = torch.nn.BatchNorm1d(32)\n",
    "        self.conv3 = MyMessagePassingLayer(32, 32, 32, 1, 0)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # convolutional part\n",
    "        x, edge_attr = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        # x, edge_attr = self.batchnorm_x(x), self.batchnorm_e(edge_attr)\n",
    "        x, edge_attr = F.leaky_relu(x), F.leaky_relu(edge_attr)\n",
    "        x, edge_attr = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        # x, edge_attr = self.batchnorm2_x(x), self.batchnorm2_e(edge_attr)\n",
    "        x, edge_attr = F.leaky_relu(x), F.leaky_relu(edge_attr)\n",
    "        x, _ = self.conv3(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGNN().to(device)\n",
    "print(model)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total nr of parameters:', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GNN before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "\n",
    "    real = batch.y[..., 0].cpu().detach().numpy()\n",
    "    pred = model(batch).cpu().detach().numpy()\n",
    "\n",
    "    plt.scatter(real, pred, s=1)\n",
    "    plt.xlabel('real')\n",
    "    plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "ep_lower_lr = 0\n",
    "\n",
    "MSE_train = []\n",
    "MSE_test = []\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    print(f'epoch {epoch:5}', end=' ')\n",
    "\n",
    "    error_train = []\n",
    "    for batch in train_loader:\n",
    "        batch = batch.clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        if torch.isnan(out).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        loss = torch.nn.MSELoss()(out, batch.y[..., 0])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        error_train.extend((out - batch.y[..., 0]).cpu().detach().numpy())\n",
    "\n",
    "    MSE_train.append(np.mean(np.asarray(error_train)**2))\n",
    "    print(f'train MSE {MSE_train[-1]:12.4}', end=' ')\n",
    "\n",
    "    model.eval()\n",
    "    error_test = []\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone().to(device)\n",
    "        out = model(batch)\n",
    "        error_test.extend((out - batch.y[..., 0]).cpu().detach().numpy())\n",
    "\n",
    "    MSE_test.append(np.mean(np.asarray(error_test)**2))\n",
    "    print(f'test MSE {MSE_test[-1]:12.4}')\n",
    "\n",
    "    # ======== Early stopping =========\n",
    "    # Stop training if loss did not decrease enough on validation data\n",
    "    if (\n",
    "        epoch > 20\n",
    "        and\n",
    "        (np.mean(MSE_test[-5:])\n",
    "             > 0.995*np.mean(MSE_test[-10:-5]))\n",
    "    ):\n",
    "\n",
    "        print('======= !! Validation loss did not decrease enough =======')\n",
    "        print('Stopping training')\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MSE_train, label='train')\n",
    "plt.plot(MSE_test, label='validation')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the real vs the predicted final node attribute, we can see that the model does manage to predict the contribution from the global average fairly accurately; its prediction is always off by about +5 or -5, which exactly what you expect if the model is averaging the possible options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "    real = batch.y[..., 0].cpu().detach().numpy()*y_std + y_m\n",
    "    pred = model(batch).cpu().detach().numpy()*y_std + y_m\n",
    "    plt.scatter(real, pred, s=1)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('real final node attribute')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.axline([0,0], [1,1], c='tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare, we show below the simplest possible prediction: we simply predict that the node attribute does not change, so the final node attribute is equal to the initial node attribute. This shows some 'fuzziness' in the prediction, which is caused by the global average contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    real = batch.y[..., 0].cpu().detach().numpy()*y_std + y_m\n",
    "    pred = batch.x.cpu().detach().numpy()*y_std + y_m\n",
    "    plt.scatter(real, pred, s=1)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.ylabel('prediction: initial node attribute')\n",
    "    plt.xlabel('real final node attribute')\n",
    "    plt.axline([0,0], [1,1], c='tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Wasserstein distance\n",
    "Since the GNN does successfully manage to predict the global average contribution but otherwise predicts the average of the possible outputs, it has an error of almost exactly 5.0 for each node. This means a single datapoint consisting of 4 nodes must move a `distance' of $\\sqrt{4\\cdot 5^2} = 10$ to end up in either of the correct solutions. Since half of the probability mass must shift toward one solution and the other half towards the other solution, everything must shift a distance of about 10. We therefore expect the Wasserstein distance to be very close to 10.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance_nd\n",
    "model.eval()\n",
    "\n",
    "rel_dist = []\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "\n",
    "    # create 1 predictions per node (since the prediction will always be the same, more samples is useless)\n",
    "    preds = model(batch).cpu().detach().numpy().reshape(-1, 1)*y_std + y_m\n",
    "\n",
    "    preds = preds.reshape(-1, 4, 1)\n",
    "    reals = batch.y.cpu().detach().numpy().reshape(-1, 4, 2)*y_std + y_m\n",
    "\n",
    "    print('preds.shape:', preds.shape)\n",
    "    print('reals.shape:', reals.shape)\n",
    "    wd = []\n",
    "    for pred, real in zip(preds, reals):  # iterate over all data points (as far as I know, this cannot be batched)\n",
    "        wd.append(wasserstein_distance_nd(pred.T, real.T))\n",
    "\n",
    "        diff = pred[..., np.newaxis] - real[..., np.newaxis, :]\n",
    "        dist = np.linalg.norm(diff, axis=0)\n",
    "        # print(diff.shape)\n",
    "        # print(dist.shape)\n",
    "        rel_dist_temp = dist[..., 0] / (dist[..., 0] + dist[..., 1])\n",
    "        # print(rel_dist_temp.shape)\n",
    "        rel_dist.extend(rel_dist_temp.flatten().tolist())\n",
    "\n",
    "print(np.mean(wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of rel_dist\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist(rel_dist,\n",
    "         bins=np.linspace(0, 1, 40), density=True)\n",
    "plt.xlabel(r'$\\frac{\\text{Distance to option 1}}{\\text{Distance to option 1 + Distance to option 2}}$', fontsize=18)\n",
    "# change label font size x label\n",
    "plt.axvline(0.0, color='red', linestyle='--')\n",
    "plt.axvline(1.0, color='red', linestyle='--')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Wasserstein distance when predicting that final = initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance_nd\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    preds = batch.x.cpu().detach().numpy().reshape(-1, 4, 1)*x_std + x_m\n",
    "    reals = batch.y.cpu().detach().numpy().reshape(-1, 4, 2)*y_std + y_m\n",
    "\n",
    "    print('preds.shape:', preds.shape)\n",
    "    print('reals.shape:', reals.shape)\n",
    "    wd = []\n",
    "    for pred, real in zip(preds, reals):  # iterate over all data points (as far as I know, this cannot be batched)\n",
    "        wd.append(wasserstein_distance_nd(pred.T, real.T))\n",
    "\n",
    "print(np.mean(wd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the Wasserstein distance is better when we use a GNN instead of just predicting that the final node value is equal to the initial node value, but not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bifurc_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
