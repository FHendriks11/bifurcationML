{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/HeadsOrTails_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data_tr = data['data_tr']\n",
    "data_te = data['data_te']\n",
    "x_std = data['x_std']\n",
    "x_m = data['x_m']\n",
    "y_std = data['y_std']\n",
    "y_m = data['y_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_tr.shape, data_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data['data_tr'], batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(data['data_te'], batch_size=100000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('cuda available')\n",
    "    # mlflow.log_param('device', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('cuda not available')\n",
    "    # mlflow.log_param('device', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(32, 32),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(32,1)\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total nr of parameters:', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    x = batch[:, 0].unsqueeze(-1).to(device=device)\n",
    "    print(x.shape)\n",
    "    y = batch[:, 1].unsqueeze(-1).to(device=device)\n",
    "    pred = model(x)\n",
    "    print('Shapes:', pred.shape, y.shape)\n",
    "\n",
    "plt.scatter(x.cpu().detach().numpy(), y.cpu().detach().numpy(), s=1, label='Real')\n",
    "plt.scatter(x.cpu().detach().numpy(), pred.cpu().detach().numpy(), s=1, label='Predicted')\n",
    "# plt.gca().set_aspect('equal')\n",
    "plt.xlabel('Bet (€)')\n",
    "plt.ylabel('Winnings (€)')\n",
    "plt.title('Real and predicted before training\\n(to check initialization of network)')\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "ep_lower_lr = 0\n",
    "\n",
    "MSE_train = []\n",
    "MSE_val = []\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    # ====================== 1) TRAIN ======================\n",
    "    model.train()\n",
    "    print(f'epoch {epoch}')\n",
    "    loss_temp = []\n",
    "    for batch in train_loader:\n",
    "        batch = batch.clone()\n",
    "        batch.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch[..., [0]].to(device))\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch[..., [1]].to(device))\n",
    "        MSE_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_temp.append(MSE_loss.item())\n",
    "\n",
    "    MSE_train.append(np.mean(loss_temp))\n",
    "    print(f' \\t train MSE loss {MSE_train[-1]:8.4}')\n",
    "\n",
    "    # ====================== 2) EVALUATE ======================\n",
    "    # In evaluation mode, so not conditioned on output. Calculating KL loss is possible for probabilistic2, but not for probabilistic1.\n",
    "    model.eval()\n",
    "    loss_temp = []\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone()\n",
    "        batch.to(device=device)\n",
    "        pred = model(batch[..., [0]].to(device))\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch[..., [0]].to(device))\n",
    "        loss_temp.append(MSE_loss.item())\n",
    "\n",
    "    MSE_val.append(np.mean(loss_temp))\n",
    "    print(f' \\t val MSE loss {MSE_val[-1]:8.4}')\n",
    "\n",
    "    # ======== Early stopping =========\n",
    "    # Stop training if both MSE and KL loss did not decrease enough on validation data\n",
    "    if (\n",
    "        epoch > 50\n",
    "        and (np.mean(MSE_val[-5:]) > 0.995*np.mean(MSE_val[-10:-5]))\n",
    "    ):\n",
    "\n",
    "        print('======= !! Validation loss did not decrease enough =======')\n",
    "        print('Stopping training')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare input, output, predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print input\n",
    "print('============== INPUT ==============')\n",
    "temp_x = (batch[:10, [0]]*data['x_std'] + data['x_m']).cpu().detach().numpy().astype(int)\n",
    "print(temp_x)\n",
    "\n",
    "print('============== OUTPUTS ==============')\n",
    "temp_y = (batch[:10, [1]]*data['y_std'] + data['y_m']).cpu().detach().numpy()\n",
    "print(temp_y)\n",
    "\n",
    "print('============== PREDICTIONS ==============')\n",
    "model.eval()\n",
    "pred = model(batch[:10, [0]].to(device))*data['y_std'] + data['y_m']\n",
    "temp_y = pred.cpu().detach().numpy()\n",
    "print(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3), dpi=200)\n",
    "\n",
    "x = (batch[:, [0]]*data['x_std'] + data['x_m']).cpu().detach().numpy()\n",
    "\n",
    "for i in range(100):\n",
    "    pred = model(batch[..., [0]].to(device))*data['y_std'] + data['y_m']\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "\n",
    "    if i == 0:\n",
    "        plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5, label='predicted')\n",
    "    else:\n",
    "        plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5)\n",
    "\n",
    "\n",
    "y = (batch[:, [1]]*data['y_std'] + data['y_m']).cpu().detach().numpy()\n",
    "plt.scatter(x, y, s=1, label='true')\n",
    "plt.xlabel(f'bet (€)')\n",
    "plt.ylabel(f'winnings (€)')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "fig.subplots_adjust(left=0.2, bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(MSE_train, label='train')\n",
    "plt.plot(MSE_val, label='val')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "\n",
    "    preds = np.zeros((len(batch), 100))  # create 100 predictions per data point\n",
    "    print(preds.shape)\n",
    "\n",
    "    for i in range(100):  # make 100 predictions for each data point\n",
    "        pred = model(batch[:, [0]])\n",
    "        preds[:, i] = pred.cpu().detach().numpy()[:, 0]*y_std+y_m\n",
    "\n",
    "    print(preds.shape)\n",
    "\n",
    "    reals = batch[:, 1:].cpu().detach().numpy()*y_std + y_m\n",
    "\n",
    "    print(reals.shape)\n",
    "\n",
    "    wd = []\n",
    "    for pred, real in zip(preds, reals):  # iterate over all data points (as far as I know, this cannot be batched)\n",
    "        wd.append(wasserstein_distance(pred, real))\n",
    "\n",
    "print('Mean Wasserstein distance between real and predicted:', np.mean(wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bifurc_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
