{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/HeadsOrTails_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data_tr = data['data_tr']\n",
    "data_te = data['data_te']\n",
    "x_m = data['x_m']\n",
    "x_std = data['x_std']\n",
    "y_m = data['y_m']\n",
    "y_std = data['y_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data['data_tr'], batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(data['data_te'], batch_size=100000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(logsig_i, mu_i, logsig_f=torch.tensor(0.0), mu_f=torch.tensor(0.0)):\n",
    "    temp = 2*(logsig_f-logsig_i) - 1 + torch.exp(logsig_i)**2/torch.exp(logsig_f)**2 + (mu_f - mu_i)**2/torch.exp(logsig_f)**2\n",
    "    temp = 0.5*torch.sum(temp, axis=-1)\n",
    "    return torch.mean(temp)\n",
    "\n",
    "def MSELoss_allTargets(pred, target, return_indices=False):\n",
    "    \"\"\"MSE loss applied on all possible targets, then taking the minimum per data point, i.e., using the closest target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : torch tensor, [N, f]\n",
    "        prediction per node, N = batch size, f = nr of predicted features\n",
    "    target : torch tensor, [N, f, a]\n",
    "        all possible targets, N = batch size, f = nr of predicted features, a = nr of possible alternatives\n",
    "    return_indices : bool, optional\n",
    "        whether to return the indices of the alternatives giving the minimum loss, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : torch tensor, [1,]\n",
    "        MSE loss, using the closest alternative for each graph\n",
    "    indices : torch tensor, [N,]\n",
    "        index indicating which of the alternatives was closest\n",
    "    \"\"\"\n",
    "    # sum square error of both nodes per alternative (resulting shape: [N, a])\n",
    "    SE = torch.sum((pred.unsqueeze(-1) - target)**2, dim=1)\n",
    "    # take the minimum square error per graph\n",
    "    SE_min, inds = torch.min(SE, axis=-1)\n",
    "\n",
    "    # take mean over entire batch\n",
    "    MSE = torch.sum(SE_min)/(pred.numel())\n",
    "\n",
    "    if return_indices:\n",
    "        return MSE, inds\n",
    "    else:\n",
    "        return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('cuda available')\n",
    "    # mlflow.log_param('device', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('cuda not available')\n",
    "    # mlflow.log_param('device', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.mlp1 = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(1, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, latent_dim*2)\n",
    "                        )\n",
    "        self.mlp2 = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(2, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, latent_dim*2)\n",
    "                        )\n",
    "        self.mlp3 = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(self.latent_dim+1, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 32),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        torch.nn.Linear(32, 1)\n",
    "                        )\n",
    "\n",
    "    def forward(self, x, y, cond_on_final=False):\n",
    "        logsig_mu_i = self.mlp1(x)\n",
    "        logsig_i, mu_i = logsig_mu_i[..., :self.latent_dim], logsig_mu_i[..., self.latent_dim:]\n",
    "        if cond_on_final:\n",
    "            # also predict f, mu based on\n",
    "            logsig_mu_f = self.mlp2(torch.cat((x, y), dim=-1))\n",
    "            logsig_f, mu_f = logsig_mu_f[..., :self.latent_dim], logsig_mu_f[..., self.latent_dim:]\n",
    "\n",
    "        # Sample\n",
    "        eps = torch.randn_like(logsig_i)\n",
    "        z_i = eps*torch.exp(logsig_i) + mu_i\n",
    "        if cond_on_final:\n",
    "            eps = torch.randn_like(logsig_f)\n",
    "            z_f = eps*torch.exp(logsig_f) + mu_f\n",
    "            x = torch.cat((x, z_f), dim=-1)\n",
    "        else:\n",
    "            x = torch.cat((x, z_i), dim=-1)\n",
    "\n",
    "        x = self.mlp3(x)\n",
    "\n",
    "        if cond_on_final:\n",
    "            return logsig_i, mu_i, logsig_f, mu_f, x\n",
    "        else:\n",
    "            return (x, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using latent_dim=2 here to be able to visualize the latent space, but a higher value would be more useful in practice\n",
    "model = MyModel(latent_dim=2).to(device=device)\n",
    "print(model)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total nr of parameters:', n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    x = batch[:, 0].unsqueeze(-1).to(device=device)\n",
    "    print(x.shape)\n",
    "    y = batch[:, 1].unsqueeze(-1).to(device=device)\n",
    "    pred = model(x, y)[-1]\n",
    "    print('Shapes:', pred.shape, y.shape)\n",
    "\n",
    "plt.scatter(x.cpu().detach().numpy(), y.cpu().detach().numpy(), s=1, label='Real')\n",
    "plt.scatter(x.cpu().detach().numpy(), pred.cpu().detach().numpy(), s=1, label='Predicted')\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone()\n",
    "    batch.to(device=device)\n",
    "    x = batch[:, 0].unsqueeze(-1).to(device=device)\n",
    "    print(x.shape)\n",
    "    y = batch[:, 1].unsqueeze(-1).to(device=device)\n",
    "    pred = model(x, y, cond_on_final=True)[-1]\n",
    "    print('Shapes:', pred.shape, y.shape)\n",
    "\n",
    "plt.scatter(x.cpu().detach().numpy(), pred.cpu().detach().numpy(), s=1, label='Predicted, cond. on output')\n",
    "plt.xlabel('Bet (€)')\n",
    "plt.ylabel('Winnings (€)')\n",
    "plt.title('Real and predicted before training\\n(to check initialization of modelwork)')\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = {\n",
    "            # on training data, while conditioning on output:\n",
    "            'MSE_train': [],\n",
    "            'KL_train': [],\n",
    "            'loss_train': [],\n",
    "\n",
    "            # on validation data, without conditioning on output:\n",
    "            'MSE_val': [],\n",
    "            'MSE_val_allTargets': [],  # MSE compared to closest target\n",
    "\n",
    "            # on validation data, but still conditioned on output:\n",
    "            'KL_val2': [],\n",
    "            'MSE_val2': []\n",
    "        }\n",
    "\n",
    "KL_increase_range = 50  # nr of epochs over which we increase the weight of the KL loss\n",
    "for epoch in range(1000):\n",
    "    print(f'epoch {epoch}')\n",
    "\n",
    "    # ====================== 1) TRAIN ======================\n",
    "    KL_weight = np.clip(epoch/KL_increase_range, a_min=0.0, a_max=1.0)/2\n",
    "    print(' \\t KL_weight:', KL_weight)\n",
    "    losses_temp = {key: [] for key in losses}\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = batch.clone().to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(batch[..., [0]], batch[..., [1]], cond_on_final=True)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch[..., [1]])\n",
    "        KL_loss1 = KL_loss(*out[:-1])\n",
    "\n",
    "        loss = (1.0-KL_weight)*MSE_loss + KL_weight*KL_loss1\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_temp['MSE_train'].append(MSE_loss.item())\n",
    "        losses_temp['KL_train'].append(KL_loss1.item())\n",
    "        losses_temp['loss_train'].append(loss.item())\n",
    "\n",
    "\n",
    "    for loss in ['MSE_train', 'KL_train', 'loss_train']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # =============== 2) EVALUATE WITHOUT CONDITIONING ON OUTPUT ===============\n",
    "    # In evaluation mode, so not conditioned on output. Calculating KL loss is therefore not possible.\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone().to(device=device)\n",
    "        out = model(batch[..., [0]], batch[..., [1]], cond_on_final=False)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch[..., [1]])\n",
    "        y_all = batch[..., 1:]\n",
    "        MSE_loss_allTargets = MSELoss_allTargets(pred, y_all.unsqueeze(1))\n",
    "\n",
    "        losses_temp['MSE_val'].append(MSE_loss.item())\n",
    "        losses_temp['MSE_val_allTargets'].append(MSE_loss_allTargets.item())\n",
    "\n",
    "    for loss in ['MSE_val', 'MSE_val_allTargets']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # ================ 3) EVALUATE WITH CONDITIONING ON OUTPUT ================\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        batch = batch.clone().to(device=device)\n",
    "        out = model(batch[..., [0]],\n",
    "                    batch[..., [1]],\n",
    "                    cond_on_final=True)\n",
    "        pred = out[-1]\n",
    "        if torch.isnan(pred).any():\n",
    "            raise ValueError('nan value in train prediction')\n",
    "\n",
    "        MSE_loss = torch.nn.MSELoss()(pred, batch[..., [1]])\n",
    "        KL_loss1 = KL_loss(*out[:-1])\n",
    "\n",
    "        losses_temp['MSE_val2'].append(MSE_loss.item())\n",
    "        losses_temp['KL_val2'].append(KL_loss1.item())\n",
    "\n",
    "    for loss in ['MSE_val2', 'KL_val2']:\n",
    "        losses[loss].append(np.mean(losses_temp[loss]))\n",
    "        print(f'\\t{loss:10} {losses[loss][-1]:8.4}')\n",
    "\n",
    "    # ======== Early stopping =========\n",
    "    # Early stopping if both MSE and KL loss did not decrease enough on validation data\n",
    "    if (\n",
    "        epoch > KL_increase_range\n",
    "        and (np.mean(losses[\"MSE_val_allTargets\"][-5:])\n",
    "             > 0.995*np.mean(losses[\"MSE_val_allTargets\"][-10:-5]))\n",
    "        and (np.mean(losses[\"KL_val2\"][-5:])\n",
    "             > 0.995*np.mean(losses[\"KL_val2\"][-10:-5]))\n",
    "    ):\n",
    "\n",
    "        print('======= !! Validation loss did not decrease enough =======')\n",
    "        print('Stopping training')\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for key, value in losses.items():\n",
    "    plt.plot(value, label=key)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare input, output, predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print input\n",
    "print('============== INPUT ==============')\n",
    "temp_x = (batch[:10, [0]]*x_std + x_m).cpu().detach().numpy().astype(int)\n",
    "print(temp_x)\n",
    "\n",
    "print('============== OUTPUTS ==============')\n",
    "temp_y = (batch[:10, [1]]*y_std + y_m).cpu().detach().numpy()\n",
    "print(temp_y)\n",
    "\n",
    "print('============== PREDICTIONS ==============')\n",
    "model.eval()\n",
    "pred = model(batch[:10, [0]].to(device), batch[:10, [1]].to(device), cond_on_final=False)[-1]*y_std + y_m\n",
    "temp_y = pred.cpu().detach().numpy()\n",
    "print(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3), dpi=200)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    x = (batch[:, [0]]*x_std + x_m).cpu().detach().numpy()\n",
    "    for i in range(10):  # 10 predictions per data point\n",
    "        batch_temp = batch.clone().to(device=device)\n",
    "        pred = model(batch_temp[..., [0]].to(device), batch_temp[..., [1]].to(device), cond_on_final=False)[-1]*y_std + y_m\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5, label='predicted')\n",
    "        else:\n",
    "            plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5)\n",
    "\n",
    "\n",
    "    x = (batch[:, [0]]*x_std + x_m).cpu().detach().numpy()\n",
    "    y = (batch[:, [1]]*y_std + y_m).cpu().detach().numpy()\n",
    "    plt.scatter(x.flatten(), y.flatten(), s=1, label='true')\n",
    "# plt.title('Not conditioned on output')\n",
    "plt.xlabel(f'bet (€)')\n",
    "plt.ylabel(f'winnings (€)')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "fig.subplots_adjust(left=0.2, bottom=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    x = (batch[:, [0]]*x_std + x_m).cpu().detach().numpy()\n",
    "    for i in range(10):\n",
    "        pred = model(batch[..., [0]].to(device), batch[..., [1]].to(device), cond_on_final=True)[-1]*y_std + y_m\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5, label='predicted')\n",
    "        else:\n",
    "            plt.scatter(x, pred, s=1, c='tab:orange', alpha=0.5)\n",
    "\n",
    "    x = (batch[:, [0]]*x_std + x_m).cpu().detach().numpy()\n",
    "    y = (batch[:, [1]]*y_std + y_m).cpu().detach().numpy()\n",
    "    plt.scatter(x, y, s=1, label='true')\n",
    "plt.title('Conditioned on output')\n",
    "plt.xlabel(f'bet (€)')\n",
    "plt.ylabel(f'winnings (€)')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 input, 200 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "N = 2000\n",
    "bet = 50.0\n",
    "x = ((torch.Tensor([[bet]]*N)-x_m)/x_std).to(device)\n",
    "y = ((torch.Tensor([[bet]]*N)-y_m)/y_std).to(device)\n",
    "model.eval()\n",
    "pred = model(x, y)[-1]*y_std + y_m\n",
    "pred = pred.cpu().detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "asdf = plt.hist(pred, bins=np.linspace(-100, 100, 50), label='predicted winnings')\n",
    "plt.xlabel('predicted winnings (€)')\n",
    "plt.ylabel('nr of predictions')\n",
    "plt.title(f'Predicted winnings for a bet of €{bet}\\n(latent space dim = {model.latent_dim})')\n",
    "plt.vlines([bet, -bet], 0, asdf[0].max(), colors='r', linestyles='dashed', label='possible ground truth winnings')\n",
    "plt.legend()\n",
    "plt.xlim(-110, 110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot latent space\n",
    "Needs a latent_dim of 2, so the latent space can be plotted in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model.latent_dim == 2:\n",
    "    raise ValueError('Plotting latent space only works for latent_dim=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "N = 300\n",
    "bet = 50.0\n",
    "x = ((torch.Tensor([[bet]]*N)-x_m)/x_std).to(device)\n",
    "y = ((torch.Tensor([[bet]]*N)-y_m)/y_std).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logsig_mu_i = model.mlp1(x)\n",
    "    logsig_i, mu_i = logsig_mu_i[..., :model.latent_dim], logsig_mu_i[..., model.latent_dim:]\n",
    "\n",
    "    logsig_mu_f = model.mlp2(torch.cat((x, y), dim=-1))\n",
    "    logsig_f, mu_f = logsig_mu_f[..., :model.latent_dim], logsig_mu_f[..., model.latent_dim:]\n",
    "    logsig_mu_f2 = model.mlp2(torch.cat((x, -y), dim=-1))\n",
    "    logsig_f2, mu_f2 = logsig_mu_f2[..., :model.latent_dim], logsig_mu_f2[..., model.latent_dim:]\n",
    "\n",
    "    # Sample\n",
    "    eps = torch.randn_like(logsig_i)\n",
    "    z_i = eps*torch.exp(logsig_i) + mu_i\n",
    "\n",
    "    eps = torch.randn_like(logsig_f)\n",
    "    z_f = eps*torch.exp(logsig_f) + mu_f\n",
    "    eps = torch.randn_like(logsig_f2)\n",
    "    z_f2 = eps*torch.exp(logsig_f2) + mu_f2\n",
    "\n",
    "    # Turn latent samples into prediction\n",
    "    x1 = torch.cat((x, z_i), dim=-1)\n",
    "    x2 = torch.cat((x, z_f), dim=-1)\n",
    "    x3 = torch.cat((x, z_f2), dim=-1)\n",
    "    pred1 = model.mlp3(x1)*y_std + y_m\n",
    "    pred2 = model.mlp3(x2)*y_std + y_m\n",
    "    pred3 = model.mlp3(x3)*y_std + y_m\n",
    "\n",
    "    # draw ellipse patch centered at mu_i with axes given by np.exp(logsig_i)\n",
    "    ellipse = mpatch.Ellipse(mu_i[0],\n",
    "                            width=2*np.exp(logsig_i.cpu().detach().numpy()[0,0]),\n",
    "                            height=2*np.exp(logsig_i.cpu().detach().numpy()[0,1]),\n",
    "                            fc='None', edgecolor='black')\n",
    "    plt.gca().add_patch(ellipse)\n",
    "    ellipse = mpatch.Ellipse(mu_f[0],\n",
    "                            width=2*np.exp(logsig_f.cpu().detach().numpy()[0,0]),\n",
    "                            height=2*np.exp(logsig_f.cpu().detach().numpy()[0,1]), fc='None', edgecolor='tab:blue',\n",
    "                            )\n",
    "    plt.gca().add_patch(ellipse)\n",
    "    ellipse = mpatch.Ellipse(mu_f2[0],\n",
    "                            width=2*np.exp(logsig_f2.cpu().detach().numpy()[0,0]),\n",
    "                            height=2*np.exp(logsig_f2.cpu().detach().numpy()[0,1]), fc='None', edgecolor='tab:orange',\n",
    "                            )\n",
    "    plt.gca().add_patch(ellipse)\n",
    "\n",
    "    # plot samples\n",
    "    plt.scatter(z_i[:, 0].cpu().detach().numpy(), z_i[:, 1].cpu().detach().numpy(), s=1, label='latent space samples', c='black')\n",
    "    plt.scatter(z_f[:, 0].cpu().detach().numpy(), z_f[:, 1].cpu().detach().numpy(), s=1, label=f'latent space samples, conditioned on {bet}', c='tab:blue')\n",
    "    plt.scatter(z_f2[:, 0].cpu().detach().numpy(), z_f2[:, 1].cpu().detach().numpy(), s=1, label=f'latent space samples, conditioned on {-bet}', c='tab:orange')\n",
    "\n",
    "    # plot means\n",
    "    plt.scatter(mu_i[:, 0].cpu().detach().numpy(), mu_i[:, 1].cpu().detach().numpy(), s=50, label='latent space means', c='black', marker='x')\n",
    "    plt.scatter(mu_f[:, 0].cpu().detach().numpy(), mu_f[:, 1].cpu().detach().numpy(), s=50, label=f'latent space means, conditioned on {bet}', c='tab:blue', marker='x')\n",
    "    plt.scatter(mu_f2[:, 0].cpu().detach().numpy(), mu_f2[:, 1].cpu().detach().numpy(), s=50, label=f'latent space means, conditioned on {-bet}', c='tab:orange', marker='x')\n",
    "\n",
    "    # contourplot of z1, z2 vs pred\n",
    "    z_temp = z_i.cpu().detach().numpy()\n",
    "    Z1, Z2 = np.meshgrid(\n",
    "        np.linspace(-3, 3, 100),\n",
    "        np.linspace(-3, 3, 100))\n",
    "    Z1 = torch.tensor(Z1).float().to(device).reshape(-1, 1)\n",
    "    Z2 = torch.tensor(Z2).float().to(device).reshape(-1, 1)\n",
    "    print(Z1.shape)\n",
    "    x_temp = ((torch.Tensor([[bet]]*len(Z1))-x_m)/x_std).to(device)\n",
    "    pred = model.mlp3(torch.cat((x_temp, Z1, Z2), dim=-1))*y_std + y_m\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "\n",
    "    plt.contourf(Z1.cpu().detach().numpy().reshape(100, 100),\n",
    "                Z2.cpu().detach().numpy().reshape(100, 100),\n",
    "                pred.reshape(100, 100),\n",
    "                levels=[-100,-75,-50,-25,0,25,50,75,100], cmap='viridis', alpha=0.5, zorder=-10)\n",
    "    plt.colorbar()\n",
    "    plt.legend()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('$z_1$')\n",
    "    plt.ylabel('$z_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.clone().to(device)\n",
    "\n",
    "    preds = np.zeros((len(batch), 100))  # create 100 predictions per data point\n",
    "    print(preds.shape)\n",
    "\n",
    "    for i in range(100):  # make 100 predictions for each data point\n",
    "        pred = model(batch[:, [0]], batch[:, [1]], cond_on_final=False)[-1]\n",
    "        preds[:, i] = pred.cpu().detach().numpy()[:, 0]*y_std+y_m\n",
    "\n",
    "    print(preds.shape)\n",
    "\n",
    "    reals = batch[:, 1:].cpu().detach().numpy()*y_std + y_m\n",
    "\n",
    "    print(reals.shape)\n",
    "\n",
    "    wd = []\n",
    "    for pred, real in zip(preds, reals):  # iterate over all data points (as far as I know, this cannot be batched)\n",
    "        wd.append(wasserstein_distance(pred, real))\n",
    "\n",
    "print('Mean Wasserstein distance between real and predicted:', np.mean(wd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bifurc_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
